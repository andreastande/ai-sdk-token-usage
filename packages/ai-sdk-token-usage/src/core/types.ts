// Specific to core, public + internal

/**
 * Represents the result state of an asynchronous token usage computation.
 *
 * This type follows the SWR-style pattern used throughout the library,
 * providing a consistent structure for all React hooks in the package.
 *
 * @typeParam T - The type of the resolved data (e.g., {@link Cost}, {@link Context}, or {@link ModelDetails}).
 *
 * @property data - The resolved data, or `undefined` if still loading or an error occurred.
 * @property isLoading - Indicates whether the data is still being loaded.
 * @property error - A {@link TokenUsageError} object if an error occurred, otherwise `null`.
 */
export type Result<T> = {
  data: T | undefined
  isLoading: boolean
  error: TokenUsageError | null
}

/**
 * Represents an error that occurred in a hook.
 *
 * All errors surfaced by hooks such as {@link useTokenContext}, {@link useTokenCost},
 * and {@link useModelDetails} are normalized to this structure. This allows a
 * consistent error-handling interface across the library, regardless of the
 * underlying error type or source.
 *
 * @property name - The name of the error (e.g., `"MissingMetadataError"`, `"ModelNotFoundError"`).
 * @property message - A human-readable description of what went wrong.
 * @property status - The numeric status code representing the error category (e.g., `400`, `404`, `500`).
 * @property info - Additional structured information about the error, such as model IDs or provider details.
 */
export type TokenUsageError = {
  name: string
  message: string
  status: number
  info: unknown
}

/**
 * Represents a raw token usage breakdown across different token categories.
 *
 * @property input - Number of standard input tokens processed by the model.
 * @property output - Number of output tokens generated by the model.
 * @property reasoning - Number of reasoning tokens used (for models that support them).
 * @property cachedInput - Number of cached input tokens, used when the provider offers token reuse.
 */
export type Breakdown = {
  input: number
  output: number
  reasoning: number
  cachedInput: number
}

/**
 * Represents the detailed cost breakdown across different token categories.
 *
 * Each property corresponds to a token type and includes both the number of
 * tokens consumed (`amount`) and the monetary cost (`cost`) for that category.
 *
 * This structure mirrors {@link Breakdown}, but augments it with cost data,
 * enabling detailed per-category cost analysis and visualization.
 *
 * @property input - Cost and token count for standard input tokens.
 * @property output - Cost and token count for output tokens generated by the model.
 * @property reasoning - Cost and token count for reasoning tokens (if supported by the model).
 * @property cachedInput - Cost and token count for cached input tokens, when token reuse is supported.
 */
export type CostBreakdown = {
  input: {
    amount: number
    cost: number
  }
  output: {
    amount: number
    cost: number
  }
  reasoning: {
    amount: number
    cost: number
  }
  cachedInput: {
    amount: number
    cost: number
  }
}

/**
 * Represents the contextual token usage information for a model response.
 *
 * This structure quantifies how much of the model’s context window is used
 * and how much remains, based on aggregated token usage data.
 *
 * It is returned by {@link useTokenContext} and used to visualize context
 * consumption, detect truncation risks, and provide real-time feedback
 * in chat or reporting interfaces.
 *
 * @property breakdown - Detailed token usage across input, output, reasoning, and cached input tokens.
 * @property used - The total number of tokens used by the model in this context window.
 * @property limit - The maximum number of tokens the model can process in its context window.
 * @property remaining - The number of tokens still available before reaching the context limit.
 * @property fractionUsed - The fraction of the context window that has been used (between `0` and `1`).
 * @property percentageUsed - The percentage of the context window that has been used (between `0` and `100`).
 * @property isExceeded - Whether the total token usage exceeds the model’s context window limit.
 */
export type Context = {
  breakdown: Breakdown
  used: number
  limit: number
  remaining: number
  fractionUsed: number
  percentageUsed: number
  isExceeded: boolean
}

/**
 * Represents the total monetary cost of token usage for a model response.
 *
 * This structure combines a detailed {@link CostBreakdown} with the aggregated
 * total cost and the currency used for pricing. It provides a unified view of
 * how much a model interaction costs, both overall and per token category.
 *
 * It is returned by {@link useTokenCost} and used to display or calculate
 * pricing information in dashboards, analytics, or chat interfaces.
 *
 * @property breakdown - Detailed per-category cost and token count information.
 * @property total - The total computed cost across all token categories.
 * @property currency - The currency code for the total cost, currently fixed to `"USD"`.
 */
export type Cost = {
  breakdown: CostBreakdown
  total: number
  currency: "USD"
}

/**
 * Represents the detailed configuration and limits of a specific model.
 *
 * This structure includes the model’s canonical identifier, per-token pricing,
 * and token limits for both context and output. It provides the necessary data
 * for computing token costs and for displaying model information in UIs.
 *
 * It is returned by {@link useModelDetails} and used internally by other hooks
 * to resolve pricing, compute costs, and calculate remaining context window capacity.
 *
 * @property canonicalSlug - The canonical model identifier, combining provider and model ID
 * (e.g., `"openai/gpt-5"`).
 * @property pricing - The per-token pricing structure for input, output, reasoning, and cached input tokens.
 * @property limit - The token limits for the model, defining both the total context window
 * (`context`) and the maximum number of output tokens (`output`).
 */
export type ModelDetails = {
  canonicalSlug: string
  pricing: Breakdown
  limit: {
    context: number
    output: number
  }
}

/** @internal */
export type NormalizedTokenUsage = Breakdown

/** @internal */
export type TokenAccountingPolicy = {
  reasoningBakedIn: boolean
}

/** @internal */
export type Provider = {
  id: string
  env: string[]
  npm: string
  api?: string
  name: string
  doc: string
  models: Record<string, Model>
}

/** @internal */
export type Model = {
  id: string
  name: string
  attachment: boolean
  reasoning: boolean
  temperature: boolean
  tool_call: boolean
  knowledge?: string
  release_date: string
  last_updated: string
  modalities: {
    input: ("text" | "audio" | "image" | "video" | "pdf")[]
    output: ("text" | "audio" | "image" | "video" | "pdf")[]
  }
  open_weights: boolean
  cost?: {
    input: number
    output: number
    reasoning?: number
    cache_read?: number
    cache_write?: number
    input_audio?: number
    output_audio?: number
  }
  limit: {
    context: number
    output: number
  }
  experimental?: boolean
  provider?: {
    npm?: string
    api?: string
  }
}

/** @internal */
export type ModelResolver = (ids: { providerId: string; modelId: string }) => Model | undefined
